{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19c3f3c8",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "[From this page](https://medium.com/@CallMeTwitch/building-a-neural-network-zoo-from-scratch-the-long-short-term-memory-network-1cec5cf31b7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "10fe6d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Imports #####\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ae6916b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(d2l.try_gpu())\n",
    "print(d2l.gpu())\n",
    "print(d2l.num_gpus())\n",
    "device = d2l.try_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "33242d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 866, Char Size: 32\n"
     ]
    }
   ],
   "source": [
    "##### Data #####\n",
    "data = \"\"\"To be, or not to be, that is the question: Whether \\\n",
    "'tis nobler in the mind to suffer The slings and arrows of ou\\\n",
    "trageous fortune, Or to take arms against a sea of troubles A\\\n",
    "nd by opposing end them. To die—to sleep, No more; and by a s\\\n",
    "leep to say we end The heart-ache and the thousand natural sh\\\n",
    "ocks That flesh is heir to: 'tis a consummation Devoutly to b\\\n",
    "e wish'd. To die, to sleep; To sleep, perchance to dream—ay, \\\n",
    "there's the rub: For in that sleep of death what dreams may c\\\n",
    "ome, When we have shuffled off this mortal coil, Must give us\\\n",
    " pause—there's the respect That makes calamity of so long lif\\\n",
    "e. For who would bear the whips and scorns of time, Th'oppres\\\n",
    "sor's wrong, the proud man's contumely, The pangs of dispriz'\\\n",
    "d love, the law's delay, The insolence of office, and the spu\\\n",
    "rns That patient merit of th'unworthy takes, When he himself \\\n",
    "might his quietus make\"\"\".lower()\n",
    "\n",
    "chars = set(data)\n",
    "\n",
    "data_size, char_size = len(data), len(chars)\n",
    "\n",
    "print(f'Data size: {data_size}, Char Size: {char_size}')\n",
    "\n",
    "char_to_idx = {c:i for i, c in enumerate(chars)}\n",
    "idx_to_char = {i:c for i, c in enumerate(chars)}\n",
    "\n",
    "train_X, train_y = data[:-1], data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a139daa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'-': 0, ' ': 1, 'c': 2, 'a': 3, ';': 4, ',': 5, '.': 6, 't': 7, 'e': 8, 'u': 9, '—': 10, 'd': 11, 'r': 12, 'f': 13, 'p': 14, 'n': 15, 'k': 16, 'b': 17, \"'\": 18, 'm': 19, 'h': 20, ':': 21, 'v': 22, 'q': 23, 'z': 24, 'o': 25, 'w': 26, 's': 27, 'i': 28, 'y': 29, 'l': 30, 'g': 31}\n",
      "{0: '-', 1: ' ', 2: 'c', 3: 'a', 4: ';', 5: ',', 6: '.', 7: 't', 8: 'e', 9: 'u', 10: '—', 11: 'd', 12: 'r', 13: 'f', 14: 'p', 15: 'n', 16: 'k', 17: 'b', 18: \"'\", 19: 'm', 20: 'h', 21: ':', 22: 'v', 23: 'q', 24: 'z', 25: 'o', 26: 'w', 27: 's', 28: 'i', 29: 'y', 30: 'l', 31: 'g'}\n"
     ]
    }
   ],
   "source": [
    "print(char_to_idx)\n",
    "print(idx_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "404622a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Helper Functions #####\n",
    "def oneHotEncode(text):\n",
    "    output = torch.zeros((char_size, 1),device=device)\n",
    "    output[char_to_idx[text]] = 1\n",
    "\n",
    "    return output\n",
    "\n",
    "# Xavier Normalized Initialization\n",
    "def initWeights(input_size:int, output_size:int) -> torch.Tensor:\n",
    "    return torch.rand((output_size, input_size),device=device).uniform_(-1, 1) # * np.sqrt(6 / (input_size + output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d50b6b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_onehot = [oneHotEncode(c) for c in train_X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "86e24ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, torch.Size([32, 1]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_X_onehot), train_X_onehot[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5988cfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(input: torch.Tensor, derivative=False):\n",
    "    if derivative:\n",
    "        return input * (1 - input)\n",
    "    return 1 / (1 + torch.exp(-input))\n",
    "\n",
    "def tanh(input:torch.Tensor, derivative=False):\n",
    "    if derivative:\n",
    "        return 1 - input ** 2\n",
    "    return torch.tanh(input)\n",
    "\n",
    "def softmax(input:torch.Tensor):\n",
    "    return torch.exp(input) / torch.sum(torch.exp(input), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a4e6331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size,num_epochs, learning_rate=0.01):\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        # Forget Gate\n",
    "        self.W_f = initWeights(input_size, hidden_size)\n",
    "        self.b_f = torch.zeros((hidden_size, 1),device=device)\n",
    "        \n",
    "        # Input Gate\n",
    "        self.W_i = initWeights(input_size, hidden_size)\n",
    "        self.b_i = torch.zeros((hidden_size, 1),device=device)\n",
    "        \n",
    "        # Candidate Gate\n",
    "        self.W_c = initWeights(input_size, hidden_size)\n",
    "        self.b_c = torch.zeros((hidden_size, 1),device=device)\n",
    "        \n",
    "        # Output Gate\n",
    "        self.W_o = initWeights(input_size, hidden_size)\n",
    "        self.b_o = torch.zeros((hidden_size, 1),device=device)\n",
    "        \n",
    "        # Hidden to Output\n",
    "        self.W_y = initWeights(hidden_size, output_size)\n",
    "        self.b_y = torch.zeros((output_size, 1),device=device)\n",
    "    def reset(self):\n",
    "        self.concat_inputs = torch.Tensor()\n",
    "        \n",
    "        self.hidden_states = {-1: torch.zeros((self.hidden_size, 1),device=device)}\n",
    "        self.cell_states = {-1: torch.zeros((self.hidden_size, 1),device=device)}\n",
    "        \n",
    "        self.activation_outputs = torch.Tensor(device=device)\n",
    "        self.cadidate_gates = torch.Tensor(device=device)\n",
    "        self.input_gates = torch.Tensor(device=device)\n",
    "        self.forget_gates = torch.Tensor(device=device)\n",
    "        self.output_gates = torch.Tensor(device=device)\n",
    "        self.outputs = torch.Tensor(device=device)\n",
    "        \n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        self.reset()\n",
    "        outputs = torch.Tensor()\n",
    "        for t in range(len(inputs)):\n",
    "            self.concat_inputs[t] = torch.concatenate((self.hidden_states[t-1], inputs[t]))      \n",
    "            \n",
    "            self.forget_gates[t] = sigmoid(self.W_f @ self.concat_inputs[t] + self.b_f)\n",
    "            self.input_gates[t] = sigmoid(self.W_i @ self.concat_inputs[t] + self.b_i)\n",
    "            self.cadidate_gates[t] = tanh(self.W_c @ self.concat_inputs[t] + self.b_c)\n",
    "            self.output_gates[t] = sigmoid(self.W_o @ self.concat_inputs[t] + self.b_o)\n",
    "           \n",
    "            self.cell_states[t] = self.forget_gates[t] * self.cell_states[t-1] + self.input_gates[t] * self.cadidate_gates[t]\n",
    "            self.hidden_states[t] = self.output_gates[t] * tanh(self.cell_states[t])\n",
    "            \n",
    "            output = self.W_y @ self.hidden_states[t] + self.b_y\n",
    "            torch.cat((outputs, output),dim=0)\n",
    "            \n",
    "        return outputs \n",
    "        \n",
    "    # Backpropagation Through Time\n",
    "    def backward(self, inputs:torch.Tensor, errors:torch.Tensor) -> None:\n",
    "        d_wf,d_bf = torch.zeros(1,device=device),torch.zeros(1,device=device)\n",
    "        d_wi,d_bi = torch.zeros(1,device=device),torch.zeros(1,device=device)\n",
    "        d_wo,d_bo = torch.zeros(1,device=device),torch.zeros(1,device=device)\n",
    "        d_wc,d_bc = torch.zeros(1,device=device),torch.zeros(1,device=device)\n",
    "        d_wy,d_by = torch.zeros(1,device=device),torch.zeros(1,device=device)\n",
    "        \n",
    "        dh_next, dc_next = torch.zeros_like(self.hidden_states[0],device=device), torch.zeros_like(self.cell_states[0],device=device)\n",
    "        for t in reversed(range(len(inputs))):\n",
    "            error = errors[t]\n",
    "            # Output layer gradients\n",
    "            \n",
    "            # Final Gate Weights and Biases Errors\n",
    "            d_wy += error @ self.hidden_states[t].T\n",
    "            d_by += error\n",
    "            \n",
    "            # Hidden State Error\n",
    "            d_hs = self.W_y.T @ error + dh_next\n",
    "            \n",
    "            # Output Gate Weights and Biases Errors\n",
    "            # Why tanh don't have derivative=True?\n",
    "            d_o = tanh(self.cell_states[t]) * d_hs * sigmoid(self.output_gates[t], derivative=True)\n",
    "            d_wo += d_o @ inputs[t].T\n",
    "            d_bo += d_o\n",
    "            \n",
    "            # Cell State Error\n",
    "            d_cs = tanh(tanh(self.cell_states[t]), derivative=True) * self.output_gates[t] * d_hs + dc_next\n",
    "            \n",
    "            # forget Gate Weights and Biases Errors\n",
    "            d_f = d_cs * self.cell_states[t-1] * sigmoid(self.forget_gates[t], derivative=True)\n",
    "            d_wf = d_f @ inputs[t].T\n",
    "            d_bf += d_f\n",
    "            \n",
    "            # Input Gate Weights and Biases Errors\n",
    "            d_i = d_cs * self.cadidate_gates[t] * sigmoid(self.input_gates[t], derivative=True)\n",
    "            d_wi += d_i @ inputs[t].T\n",
    "            d_bi += d_i\n",
    "            \n",
    "            # Candidate Gate Weights and Biases Errors\n",
    "            d_c = d_cs * self.input_gates[t] * tanh(self.cadidate_gates[t], derivative=True)\n",
    "            d_wc += d_c @ inputs[t].T\n",
    "            d_bc += d_c\n",
    "            \n",
    "            # Concanted Input Error (Sum of Error for each gate)\n",
    "            d_z = self.W_f.T @ d_f + self.W_i.T @ d_i + self.W_c.T @ d_c + self.W_o.T @ d_o\n",
    "            \n",
    "            # Error of Hidden State and Cell State at Next Time Step\n",
    "            dh_next = d_z[:self.hidden_size, :]\n",
    "            dc_next = self.forget_gates[t] * d_cs\n",
    "            \n",
    "        for d_ in [d_wf,d_bf,d_wi,d_bi,d_wo,d_bo,d_wc,d_bc,d_wy,d_by]:\n",
    "            torch.clamp(d_, min = -1,max= 1, out=d_)\n",
    "            \n",
    "        # Update Weights and Biases\n",
    "        \n",
    "        self.W_f += self.learning_rate * d_wf\n",
    "        self.b_f += self.learning_rate * d_bf\n",
    "        \n",
    "        self.W_i += self.learning_rate * d_wi\n",
    "        self.b_i += self.learning_rate * d_bi\n",
    "        \n",
    "        self.W_o += self.learning_rate * d_wo\n",
    "        self.b_o += self.learning_rate * d_bo\n",
    "        \n",
    "        self.W_c += self.learning_rate * d_wc\n",
    "        self.b_c += self.learning_rate * d_bc\n",
    "        \n",
    "        self.W_y += self.learning_rate * d_wy\n",
    "        self.b_y += self.learning_rate * d_by    \n",
    "        \n",
    "    # Training the Model\n",
    "    def train(self, inputs: torch.Tensor, targets: torch.Tensor) -> None:\n",
    "        \n",
    "        for _ in tqdm(range(self.num_epochs)):\n",
    "            predictions = self.forward(inputs)\n",
    "            \n",
    "            errors = torch.Tensor(device=device)\n",
    "            \n",
    "            for t in range(len(predictions)):\n",
    "                errors = torch.cat((errors, -softmax(predictions[t])), dim=0)\n",
    "                errors[-1][targets[t]] += 1\n",
    "            self.backward(errors, self.concat_inputs)\n",
    "\n",
    "    # Test\n",
    "\n",
    "    def test(self, inputs: torch.Tensor, targets: torch.Tensor) -> None:\n",
    "        accuracy = 0\n",
    "        probabilities = self.forward(inputs)\n",
    "        \n",
    "        output = []\n",
    "        \n",
    "        for q in range(len(targets)):\n",
    "            prediction = softmax(probabilities[q])\n",
    "            output += prediction\n",
    "            \n",
    "            if torch.argmax(prediction) == targets[q]:\n",
    "                accuracy += 1\n",
    "            \n",
    "            print(f'Ground Truth: {targets[q]}, Prediction: {torch.argmax(prediction).item()}')\n",
    "            print(f'Accuracy: {accuracy / len(targets) * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3e7f18ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Network\n",
    "hidden_size = 25\n",
    "\n",
    "lstm = LSTM(input_size = char_size + hidden_size, hidden_size = hidden_size, output_size = char_size, num_epochs = 1_000, learning_rate = 0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a8a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_onehot = [oneHotEncode(c) for c in train_X]\n",
    "train_y_onehot = [oneHotEncode(c) for c in train_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91edadec",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Training #####\n",
    "lstm.train(train_X_onehot, train_y_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0de68d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy 2D dot product (matrix multiplication): \n",
      "[[19 22]\n",
      " [43 50]]\n",
      "PyTorch 2D @ operator: \n",
      "tensor([[19, 22],\n",
      "        [43, 50]])\n",
      "PyTorch 2D matmul: \n",
      "tensor([[19, 22],\n",
      "        [43, 50]])\n"
     ]
    }
   ],
   "source": [
    "A_np = np.array([[1, 2], [3, 4]])\n",
    "B_np = np.array([[5, 6], [7, 8]])\n",
    "result_np = np.dot(A_np, B_np)\n",
    "print(f\"NumPy 2D dot product (matrix multiplication): \\n{result_np}\")\n",
    "\n",
    "A_torch = torch.tensor([[1, 2], [3, 4]])\n",
    "B_torch = torch.tensor([[5, 6], [7, 8]])\n",
    "result_torch_matmul = torch.matmul(A_torch, B_torch)\n",
    "result_torch_at = A_torch @ B_torch\n",
    "print(f\"PyTorch 2D @ operator: \\n{result_torch_at}\")\n",
    "print(f\"PyTorch 2D matmul: \\n{result_torch_matmul}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dba693f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_torch = torch.cat((A_torch, B_torch), dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b2b29c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 8])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "A_torch[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
